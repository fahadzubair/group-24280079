{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3984a74-2b74-41da-8348-374cda44c2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/13 12:04:04 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"RealtimeTrafficAnalysis\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b38643d7-f908-4f21-b4d4-87cdf46929cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-------------+-------------+----------------+\n",
      "|sensor_id|timestamp          |vehicle_count|average_speed|congestion_level|\n",
      "+---------+-------------------+-------------+-------------+----------------+\n",
      "|101      |2025-05-13T14:57:38|26           |46.21        |MEDIUM          |\n",
      "|102      |2025-05-13T14:57:38|32           |72.16        |MEDIUM          |\n",
      "|103      |2025-05-13T14:57:38|13           |66.13        |MEDIUM          |\n",
      "|104      |2025-05-13T14:57:38|27           |12.43        |HIGH            |\n",
      "|105      |2025-05-13T14:57:38|42           |36.23        |MEDIUM          |\n",
      "|101      |2025-05-13T14:57:39|34           |38.31        |LOW             |\n",
      "|102      |2025-05-13T14:57:39|17           |99.52        |LOW             |\n",
      "|103      |2025-05-13T14:57:39|48           |40.41        |MEDIUM          |\n",
      "|104      |2025-05-13T14:57:39|0            |10.94        |HIGH            |\n",
      "|105      |2025-05-13T14:57:39|2            |74.8         |MEDIUM          |\n",
      "|101      |2025-05-13T14:57:40|30           |93.27        |HIGH            |\n",
      "|102      |2025-05-13T14:57:40|17           |57.02        |LOW             |\n",
      "|103      |2025-05-13T14:57:40|50           |24.78        |LOW             |\n",
      "|104      |2025-05-13T14:57:40|8            |29.29        |MEDIUM          |\n",
      "|105      |2025-05-13T14:57:40|20           |15.83        |MEDIUM          |\n",
      "|101      |2025-05-13T14:57:41|0            |89.8         |HIGH            |\n",
      "|102      |2025-05-13T14:57:41|9            |38.98        |MEDIUM          |\n",
      "|103      |2025-05-13T14:57:41|22           |36.67        |LOW             |\n",
      "|104      |2025-05-13T14:57:41|47           |42.37        |HIGH            |\n",
      "|105      |2025-05-13T14:57:41|36           |17.59        |HIGH            |\n",
      "+---------+-------------------+-------------+-------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- sensor_id: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- vehicle_count: integer (nullable = true)\n",
      " |-- average_speed: double (nullable = true)\n",
      " |-- congestion_level: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"sensor_id\", StringType(), True),\n",
    "        StructField(\"timestamp\", StringType(), True),\n",
    "        StructField(\"vehicle_count\", IntegerType(), True),\n",
    "        StructField(\"average_speed\", DoubleType(), True),\n",
    "        StructField(\"congestion_level\", StringType(), True),\n",
    "    ]\n",
    ")\n",
    "df = spark.read.schema(schema).json(\"data.json\")\n",
    "\n",
    "df.show(truncate=False)\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "226b5cb9-c487-4a40-9b34-e8d254888a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enforce schema validation\n",
    "df = df.filter(\n",
    "    col(\"sensor_id\").isNotNull() & \n",
    "    col(\"timestamp\").isNotNull() & \n",
    "    (col(\"vehicle_count\") >= 0) & \n",
    "    (col(\"average_speed\") > 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7201bf8-ccda-4d9b-8a19-cb575ac6ecc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle duplicate records using deduplication (sensor_id + timestamp)\n",
    "deduplicated_df = validated_df.dropDuplicates([\"sensor_id\", \"timestamp\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "286f56ca-618b-4f9f-82fd-7471726e7212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+---------+-------------------+\n",
      "|window                                    |sensor_id|total_vehicle_count|\n",
      "+------------------------------------------+---------+-------------------+\n",
      "|{2025-05-13 14:57:00, 2025-05-13 14:58:00}|105      |530                |\n",
      "|{2025-05-13 14:59:00, 2025-05-13 15:00:00}|104      |1484               |\n",
      "|{2025-05-13 15:00:00, 2025-05-13 15:01:00}|103      |433                |\n",
      "|{2025-05-13 14:57:00, 2025-05-13 14:58:00}|101      |542                |\n",
      "|{2025-05-13 14:59:00, 2025-05-13 15:00:00}|102      |1414               |\n",
      "|{2025-05-13 14:59:00, 2025-05-13 15:00:00}|101      |1434               |\n",
      "|{2025-05-13 14:58:00, 2025-05-13 14:59:00}|105      |1677               |\n",
      "|{2025-05-13 15:00:00, 2025-05-13 15:01:00}|101      |522                |\n",
      "|{2025-05-13 14:58:00, 2025-05-13 14:59:00}|101      |1483               |\n",
      "|{2025-05-13 14:57:00, 2025-05-13 14:58:00}|103      |475                |\n",
      "|{2025-05-13 14:57:00, 2025-05-13 14:58:00}|104      |545                |\n",
      "|{2025-05-13 14:59:00, 2025-05-13 15:00:00}|105      |1409               |\n",
      "|{2025-05-13 15:00:00, 2025-05-13 15:01:00}|104      |575                |\n",
      "|{2025-05-13 15:00:00, 2025-05-13 15:01:00}|105      |522                |\n",
      "|{2025-05-13 14:57:00, 2025-05-13 14:58:00}|102      |549                |\n",
      "|{2025-05-13 14:58:00, 2025-05-13 14:59:00}|104      |1406               |\n",
      "|{2025-05-13 14:58:00, 2025-05-13 14:59:00}|103      |1603               |\n",
      "|{2025-05-13 14:58:00, 2025-05-13 14:59:00}|102      |1651               |\n",
      "|{2025-05-13 15:00:00, 2025-05-13 15:01:00}|102      |737                |\n",
      "|{2025-05-13 14:59:00, 2025-05-13 15:00:00}|103      |1466               |\n",
      "+------------------------------------------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Traffic Volume per Sensor (5-minute window)\n",
    "traffic_volume = df.groupBy(\n",
    "    window(\"timestamp\", \"1 minutes\"),\n",
    "    \"sensor_id\"\n",
    ").agg(sum(\"vehicle_count\").alias(\"total_vehicle_count\"))\n",
    "\n",
    "\n",
    "traffic_volume.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b45e14d3-dec2-49f4-8b1a-d4e049717d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+\n",
      "|sensor_id|timestamp          |\n",
      "+---------+-------------------+\n",
      "|101      |2025-05-13T14:57:42|\n",
      "|101      |2025-05-13T14:59:33|\n",
      "|102      |2025-05-13T14:58:28|\n",
      "|102      |2025-05-13T14:59:09|\n",
      "|102      |2025-05-13T14:59:10|\n",
      "|102      |2025-05-13T14:59:11|\n",
      "|103      |2025-05-13T14:57:51|\n",
      "|104      |2025-05-13T14:57:59|\n",
      "|104      |2025-05-13T14:58:00|\n",
      "|104      |2025-05-13T14:58:06|\n",
      "|104      |2025-05-13T14:59:19|\n",
      "|104      |2025-05-13T14:59:57|\n",
      "|104      |2025-05-13T15:00:10|\n",
      "|104      |2025-05-13T15:00:11|\n",
      "|105      |2025-05-13T14:57:48|\n",
      "|105      |2025-05-13T14:57:49|\n",
      "|105      |2025-05-13T14:58:31|\n",
      "|105      |2025-05-13T14:58:56|\n",
      "|105      |2025-05-13T14:58:57|\n",
      "|105      |2025-05-13T14:58:58|\n",
      "+---------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Congestion Hotspots (3 consecutive HIGH congestion levels)\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_spec = Window.partitionBy(\"sensor_id\").orderBy(\"timestamp\")\n",
    "parsed_df = df.withColumn(\n",
    "    \"prev_congestion\",\n",
    "    lag(\"congestion_level\", 1).over(window_spec)\n",
    ").withColumn(\n",
    "    \"prev2_congestion\",\n",
    "    lag(\"congestion_level\", 2).over(window_spec)\n",
    ")\n",
    "congestion_hotspots = parsed_df.filter(\n",
    "    (col(\"congestion_level\") == \"HIGH\") &\n",
    "    (col(\"prev_congestion\") == \"HIGH\") &\n",
    "    (col(\"prev2_congestion\") == \"HIGH\")\n",
    ").select(\"sensor_id\", \"timestamp\")\n",
    "\n",
    "congestion_hotspots.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2c2e9416-d90d-45fd-bd57-6cca8b165901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+---------+------------------+\n",
      "|window                                    |sensor_id|avg_speed         |\n",
      "+------------------------------------------+---------+------------------+\n",
      "|{2025-05-13 14:50:00, 2025-05-13 15:00:00}|104      |56.27680851063827 |\n",
      "|{2025-05-13 14:55:00, 2025-05-13 15:05:00}|105      |53.68239263803681 |\n",
      "|{2025-05-13 15:00:00, 2025-05-13 15:10:00}|101      |51.91130434782608 |\n",
      "|{2025-05-13 14:55:00, 2025-05-13 15:05:00}|103      |55.08380368098162 |\n",
      "|{2025-05-13 15:00:00, 2025-05-13 15:10:00}|103      |57.73545454545455 |\n",
      "|{2025-05-13 15:00:00, 2025-05-13 15:10:00}|104      |51.16454545454545 |\n",
      "|{2025-05-13 14:50:00, 2025-05-13 15:00:00}|101      |54.07262411347518 |\n",
      "|{2025-05-13 14:50:00, 2025-05-13 15:00:00}|102      |58.01             |\n",
      "|{2025-05-13 14:50:00, 2025-05-13 15:00:00}|103      |54.67007092198583 |\n",
      "|{2025-05-13 14:55:00, 2025-05-13 15:05:00}|104      |55.58680981595089 |\n",
      "|{2025-05-13 14:55:00, 2025-05-13 15:05:00}|102      |56.8215337423313  |\n",
      "|{2025-05-13 14:50:00, 2025-05-13 15:00:00}|105      |52.820567375886526|\n",
      "|{2025-05-13 15:00:00, 2025-05-13 15:10:00}|105      |59.2059090909091  |\n",
      "|{2025-05-13 14:55:00, 2025-05-13 15:05:00}|101      |53.76951219512195 |\n",
      "|{2025-05-13 15:00:00, 2025-05-13 15:10:00}|102      |49.204545454545446|\n",
      "+------------------------------------------+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Average Speed per Sensor (10-minute rolling window)\n",
    "average_speed = parsed_df.groupBy(\n",
    "    window(\"timestamp\", \"10 minutes\", \"5 minutes\"),\n",
    "    \"sensor_id\"\n",
    ").agg(avg(\"average_speed\").alias(\"avg_speed\"))\n",
    "average_speed.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "653c4727-9977-479a-9d0b-f180cee0c97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-------------+-------------+----------------+----------+------------+-------+\n",
      "|sensor_id|timestamp          |vehicle_count|average_speed|congestion_level|prev_speed|speed_drop_%|row_num|\n",
      "+---------+-------------------+-------------+-------------+----------------+----------+------------+-------+\n",
      "|101      |2025-05-13T14:59:39|5            |78.85        |HIGH            |14.78     |433.49      |1      |\n",
      "|101      |2025-05-13T14:59:25|3            |83.13        |MEDIUM          |16.86     |393.06      |2      |\n",
      "|101      |2025-05-13T14:58:49|35           |56.35        |MEDIUM          |11.69     |382.04      |3      |\n",
      "|101      |2025-05-13T14:59:50|39           |63.11        |MEDIUM          |13.98     |351.43      |4      |\n",
      "|102      |2025-05-13T14:59:28|46           |82.66        |LOW             |11.61     |611.97      |1      |\n",
      "|102      |2025-05-13T14:57:53|19           |76.71        |MEDIUM          |10.97     |599.27      |2      |\n",
      "|102      |2025-05-13T15:00:02|50           |66.54        |HIGH            |10.42     |538.58      |3      |\n",
      "|102      |2025-05-13T14:59:15|35           |64.72        |LOW             |10.99     |488.9       |4      |\n",
      "|103      |2025-05-13T14:59:35|38           |92.63        |LOW             |13.3      |596.47      |1      |\n",
      "|103      |2025-05-13T14:59:43|49           |80.14        |HIGH            |12.66     |533.02      |2      |\n",
      "|103      |2025-05-13T14:57:43|11           |60.83        |HIGH            |10.19     |496.96      |3      |\n",
      "|103      |2025-05-13T14:59:31|22           |88.08        |LOW             |18.14     |385.56      |4      |\n",
      "|104      |2025-05-13T15:00:16|9            |70.02        |MEDIUM          |11.57     |505.19      |1      |\n",
      "|104      |2025-05-13T14:58:20|32           |62.44        |HIGH            |10.51     |494.1       |2      |\n",
      "|104      |2025-05-13T14:59:37|16           |96.89        |LOW             |17.09     |466.94      |3      |\n",
      "|104      |2025-05-13T14:59:29|49           |92.59        |HIGH            |17.21     |438.0       |4      |\n",
      "|105      |2025-05-13T14:58:20|16           |84.66        |LOW             |11.48     |637.46      |1      |\n",
      "|105      |2025-05-13T14:58:58|13           |93.1         |HIGH            |14.78     |529.91      |2      |\n",
      "|105      |2025-05-13T14:57:50|6            |79.1         |MEDIUM          |13.65     |479.49      |3      |\n",
      "|105      |2025-05-13T14:57:44|27           |90.66        |HIGH            |17.94     |405.35      |4      |\n",
      "+---------+-------------------+-------------+-------------+----------------+----------+------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# **Sudden Speed Drop Detection**: drop > 50% within **2 mins**.\n",
    "window_spec = Window.partitionBy(\"sensor_id\").orderBy(\"timestamp\")\n",
    "speed_drop = df.withColumn(\n",
    "    \"prev_speed\", \n",
    "    lag(\"average_speed\", 1).over(window_spec)\n",
    ").withColumn(\n",
    "    \"speed_drop_%\", \n",
    "    round(abs((col(\"average_speed\") - col(\"prev_speed\")) / col(\"prev_speed\") * 100), 2)\n",
    ").filter(\n",
    "    col(\"speed_drop_%\") > 50\n",
    ")\n",
    "rank_window_spec = Window.partitionBy(\"sensor_id\").orderBy(col(\"speed_drop_%\").desc())\n",
    "top5_speed_drops = speed_drop.select(\n",
    "    \"*\",\n",
    "    rank().over(rank_window_spec).alias(\"row_num\")\n",
    ").filter(col(\"row_num\").isin([1,2,3,4])).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ae684921-6527-4d49-af16-e03d66b56a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+---------+-------------------+\n",
      "|window                                    |sensor_id|total_vehicle_count|\n",
      "+------------------------------------------+---------+-------------------+\n",
      "|{2025-05-13 14:58:00, 2025-05-13 15:00:00}|105      |3086               |\n",
      "|{2025-05-13 14:58:00, 2025-05-13 15:00:00}|103      |3069               |\n",
      "|{2025-05-13 14:58:00, 2025-05-13 15:00:00}|102      |3065               |\n",
      "+------------------------------------------+---------+-------------------+\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "Failed to find data source: kafka. Please deploy the application as per the deployment section of Structured Streaming + Kafka Integration Guide.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m      4\u001b[39m busiest_sensors = df.groupBy(\n\u001b[32m      5\u001b[39m     window(\u001b[33m\"\u001b[39m\u001b[33mtimestamp\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m2 minutes\u001b[39m\u001b[33m\"\u001b[39m), \n\u001b[32m      6\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msensor_id\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m ).agg(\u001b[38;5;28msum\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mvehicle_count\u001b[39m\u001b[33m\"\u001b[39m).alias(\u001b[33m\"\u001b[39m\u001b[33mtotal_vehicle_count\u001b[39m\u001b[33m\"\u001b[39m)).orderBy(\n\u001b[32m      8\u001b[39m     col(\u001b[33m\"\u001b[39m\u001b[33mtotal_vehicle_count\u001b[39m\u001b[33m\"\u001b[39m).desc()\n\u001b[32m      9\u001b[39m ).limit(\u001b[32m3\u001b[39m)\n\u001b[32m     11\u001b[39m busiest_sensors.show(\n\u001b[32m     12\u001b[39m     truncate=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     13\u001b[39m )\n\u001b[32m     14\u001b[39m \u001b[43mbusiest_sensors\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselectExpr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mto_json(struct(*)) AS value\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     16\u001b[39m \u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m  \u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mkafka\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m  \u001b[49m\u001b[43m.\u001b[49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mkafka.bootstrap.servers\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlocalhost:9092\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m  \u001b[49m\u001b[43m.\u001b[49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtopic\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtraffic_analysis\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[43m  \u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~opt/bitnami/spark/python/pyspark/sql/readwriter.py:1461\u001b[39m, in \u001b[36mDataFrameWriter.save\u001b[39m\u001b[34m(self, path, format, mode, partitionBy, **options)\u001b[39m\n\u001b[32m   1459\u001b[39m     \u001b[38;5;28mself\u001b[39m.format(\u001b[38;5;28mformat\u001b[39m)\n\u001b[32m   1460\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1461\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1462\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1463\u001b[39m     \u001b[38;5;28mself\u001b[39m._jwrite.save(path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~.local/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~opt/bitnami/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    181\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: Failed to find data source: kafka. Please deploy the application as per the deployment section of Structured Streaming + Kafka Integration Guide."
     ]
    }
   ],
   "source": [
    "# **Top 3 Busiest Sensors**: by vehicle count in **last 30 mins**.\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "busiest_sensors = df.groupBy(\n",
    "    window(\"timestamp\", \"2 minutes\"), \n",
    "    \"sensor_id\"\n",
    ").agg(sum(\"vehicle_count\").alias(\"total_vehicle_count\")).orderBy(\n",
    "    col(\"total_vehicle_count\").desc()\n",
    ").limit(3)\n",
    "    \n",
    "busiest_sensors.show(\n",
    "    truncate=False\n",
    ")\n",
    "busiest_sensors.selectExpr(\n",
    "    \"to_json(struct(*)) AS value\"\n",
    ").write \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "  .option(\"topic\", \"traffic_analysis\") \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "af45a059-bff3-43e0-afc0-f239991eae58",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Failed to find data source: kafka. Please deploy the application as per the deployment section of Structured Streaming + Kafka Integration Guide.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[60]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mkafka\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~opt/bitnami/spark/python/pyspark/sql/readwriter.py:314\u001b[39m, in \u001b[36mDataFrameReader.load\u001b[39m\u001b[34m(self, path, format, schema, **options)\u001b[39m\n\u001b[32m    312\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28mself\u001b[39m._jreader.load(\u001b[38;5;28mself\u001b[39m._spark._sc._jvm.PythonUtils.toSeq(path)))\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~.local/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~opt/bitnami/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    181\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: Failed to find data source: kafka. Please deploy the application as per the deployment section of Structured Streaming + Kafka Integration Guide."
     ]
    }
   ],
   "source": [
    "\n",
    "spark.read.format(\"kafka\").load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7e495e-9048-4a89-b26c-f352dec4912e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
